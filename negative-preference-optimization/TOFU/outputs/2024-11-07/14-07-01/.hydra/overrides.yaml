- split=Ancient_Egypt
- batch_size=4
- gradient_accumulation_steps=4
- model_family=llama3-8b-chat
- lr=
